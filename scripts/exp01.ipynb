{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "train_path = '../data/train_data.json'\n",
    "dev_path = '../data/dev_data.json'\n",
    "test_path = '../data/test1_data_postag.json'\n",
    "schema_path = '../data/all_50_schemas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "train = []\n",
    "with open(train_path, 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "dev = []\n",
    "with open(dev_path, 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        dev.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "test = []\n",
    "with open(test_path, 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        test.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 信息抽取数据集 EDA\n",
    "- 将数据整合为Dataframe格式，便于后续分析优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(data):\n",
    "    text = []\n",
    "    spo_list = []\n",
    "    postag = []\n",
    "    for sample in data:\n",
    "        text_ = sample['text']\n",
    "        postag_ = sample.get('postag', [])\n",
    "        spo_list_ = sample.get('spo_list', [])\n",
    "        text.append(text_)\n",
    "        spo_list.append(json.dumps(spo_list_, ensure_ascii=False))\n",
    "        postag.append(json.dumps(postag_, ensure_ascii=False))\n",
    "    return pd.DataFrame(data={'text': text, 'spo_list': spo_list, 'postag': postag})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = create_df(train)\n",
    "dev_df = create_df(dev)\n",
    "test_df = create_df(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理官方postag数据\n",
    "- words：官方分词\n",
    "- poses：官方pos\n",
    "- has_postag: 是否提供postag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(postag):\n",
    "    pt = json.loads(postag)\n",
    "    words = [x['word'] for x in pt]\n",
    "    return '\\n'.join(words)\n",
    "def get_poses(postag):\n",
    "    pt = json.loads(postag)\n",
    "    poses = [x['pos'] for x in pt]\n",
    "    return ' '.join(poses)\n",
    "def has_postag(postag):\n",
    "    pt = json.loads(postag)\n",
    "    return len(pt) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_postag(df):\n",
    "    df['words'] = df.apply(lambda x: get_words(x['postag']), axis=1)\n",
    "    df['poses'] = df.apply(lambda x: get_poses(x['postag']), axis=1)\n",
    "    df['has_postag'] = df.apply(lambda x: has_postag(x['postag']), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = process_postag(train_df)\n",
    "dev_df = process_postag(dev_df)\n",
    "test_df = process_postag(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理SPO数据\n",
    "- spo: 将spo_list 转换为 [((sub_s, sub_e), (obj_s, obj_e), rel_id), ...] 格式 (sub_s 为subject在text开始位置)\n",
    "- no_ref_sub: 存在subject未出现在text\n",
    "- no_ref_obj: 存在object未出现在text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema2id():\n",
    "    id2schema = []\n",
    "    with open('schema_vocab.txt', 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            schema = line.split('\\t')[0]\n",
    "            id2schema.append(schema)\n",
    "    schema2id = {}\n",
    "    for i in range(len(id2schema)):\n",
    "        schema2id[id2schema[i]] = i\n",
    "    return schema2id\n",
    "schema2id = get_schema2id()\n",
    "def get_tri(raw_tri, text):\n",
    "    \"\"\"\n",
    "    [((sub_s, sub_e), (obj_s, obj_e), rel_id), ...]\n",
    "    \"\"\"\n",
    "    pro_tri = []\n",
    "    for t in raw_tri:\n",
    "        sub = t['subject'].lower()\n",
    "        obj = t['object'].lower()\n",
    "        pre = t['predicate']\n",
    "        pid = schema2id[pre]\n",
    "        subs = text.find(sub)\n",
    "        sube = subs + len(sub) - 1 if subs != -1 else subs\n",
    "        objs = text.find(obj)\n",
    "        obje = objs + len(obj) - 1 if objs != -1 else objs\n",
    "        pro_tri.append(((subs, sube), (objs, obje), pid))\n",
    "    return json.dumps(pro_tri, ensure_ascii=False)\n",
    "def no_ref_sub(tri):\n",
    "    for t in tri:\n",
    "        subs = t[0][0]\n",
    "        if subs == -1:\n",
    "            return True\n",
    "    return False\n",
    "def no_ref_obj(tri):\n",
    "    for t in tri:\n",
    "        objs = t[1][0]\n",
    "        if objs == -1:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tri(df):\n",
    "    df['spo'] = df.apply(lambda x: get_tri(json.loads(x['spo_list']), x['text'].lower()), axis=1)\n",
    "    df['no_ref_sub'] = df.apply(lambda x: no_ref_sub(json.loads(x['spo'])), axis=1)\n",
    "    df['no_ref_obj'] = df.apply(lambda x: no_ref_obj(json.loads(x['spo'])), axis=1)\n",
    "    df['num_spo'] = df.apply(lambda x: len(json.loads(x['spo'])), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = process_tri(train_df)\n",
    "dev_df = process_tri(dev_df)\n",
    "test_df = process_tri(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修正训练集当中 spo实体未出现在text中的label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postag</th>\n",
       "      <th>spo_list</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>poses</th>\n",
       "      <th>has_postag</th>\n",
       "      <th>spo</th>\n",
       "      <th>no_ref_sub</th>\n",
       "      <th>no_ref_obj</th>\n",
       "      <th>num_spo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [postag, spo_list, text, words, poses, has_postag, spo, no_ref_sub, no_ref_obj, num_spo]\n",
       "Index: []"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_df[(train_df['no_ref_obj'] == True) | (train_df['no_ref_sub'] == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "def analy_spo(index):\n",
    "    data = train_df.iloc[index]\n",
    "    text = data['text']\n",
    "    print(f'index:{index}')\n",
    "    print(f'text:{text}')\n",
    "    sl, sp = json.loads(data['spo_list']), json.loads(data['spo'])\n",
    "    for i in range(len(sl)):\n",
    "        spo = sp[i]\n",
    "        if spo[0][0] == -1 or spo[1][0] == -1:\n",
    "            print('\\nNOREF:')\n",
    "            pp.pprint(sl[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "indexs = list(train_df[(train_df['no_ref_obj'] == True) | (train_df['no_ref_sub'] == True)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "train_df.loc[16866,'spo_list'] = '[{\"predicate\": \"作者\", \"object_type\": \"人物\", \"subject_type\": \"图书作品\", \"object\": \"胡抗美\", \"subject\": \"康里巎巎杂诗(全彩色高清珍藏本)\"}]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "#, {\"predicate\": \"改编自\", \"object_type\": \"作品\", \"subject_type\": \"影视作品\", \"object\": \"裸婚——80后的新结婚时代\", \"subject\": \"裸婚时代\"}\n",
    "train_df.loc[32313,'spo_list'] = '[{\"predicate\": \"主演\", \"object_type\": \"人物\", \"subject_type\": \"影视作品\", \"object\": \"张萌\", \"subject\": \"沙海\"}, {\"predicate\": \"主演\", \"object_type\": \"人物\", \"subject_type\": \"影视作品\", \"object\": \"吴磊\", \"subject\": \"斗破苍穹\"}, {\"predicate\": \"主演\", \"object_type\": \"人物\", \"subject_type\": \"影视作品\", \"object\": \"朱杰\", \"subject\": \"沙海\"}, {\"predicate\": \"作者\", \"object_type\": \"人物\", \"subject_type\": \"图书作品\", \"object\": \"南派三叔\", \"subject\": \"沙海\"}, {\"predicate\": \"主演\", \"object_type\": \"人物\", \"subject_type\": \"影视作品\", \"object\": \"张铭恩\", \"subject\": \"沙海\"}]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "train_df.loc[82535,'spo_list'] = '[{\"predicate\": \"主持人\", \"object_type\": \"人物\", \"subject_type\": \"电视综艺\", \"object\": \"王立群\", \"subject\": \"百家讲坛\"}]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "train_df.loc[93654,'spo_list'] = '[{\"predicate\": \"编剧\", \"object_type\": \"人物\", \"subject_type\": \"影视作品\", \"object\": \"威廉·莫纳汉\", \"subject\": \"天国王朝\"}, {\"predicate\": \"编剧\", \"object_type\": \"人物\", \"subject_type\": \"影视作品\", \"object\": \"威廉·莫纳汉\", \"subject\": \"无间行者\"}]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "train_df.loc[99891,'text'] = '概述桂林国际会展中心位于素有百里画廊之称的漓江之滨，是桂林市标志性建筑，占地面积15万平方米，建筑总面积5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "train_df.loc[138087,'spo_list'] = '[{\"predicate\": \"出版社\", \"object_type\": \"出版社\", \"subject_type\": \"书籍\", \"object\": \"人民文学出版社\", \"subject\": \"千字文全解\"}]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "train_df.loc[152858,'spo_list'] = '[{\"predicate\": \"作者\", \"object_type\": \"人物\", \"subject_type\": \"图书作品\", \"object\": \"马银琴\", \"subject\": \"周秦时代诗的传播史\"}, {\"predicate\": \"出版社\", \"object_type\": \"出版社\", \"subject_type\": \"书籍\", \"object\": \"社会科学文献出版社\", \"subject\": \"周秦时代诗的传播史\"}]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "train_df.loc[167561,'spo_list'] = '[{\"predicate\": \"主演\", \"object_type\": \"人物\", \"subject_type\": \"影视作品\", \"object\": \"万茜\", \"subject\": \"裸婚时代\"}, {\"predicate\": \"出品公司\", \"object_type\": \"企业\", \"subject_type\": \"影视作品\", \"object\": \"北京光彩世纪文化艺术有限公司\", \"subject\": \"裸婚时代\"}, {\"predicate\": \"导演\", \"object_type\": \"人物\", \"subject_type\": \"影视作品\", \"object\": \"滕华涛\", \"subject\": \"裸婚时代\"}, {\"predicate\": \"主演\", \"object_type\": \"人物\", \"subject_type\": \"影视作品\", \"object\": \"韩童生\", \"subject\": \"裸婚时代\"}, {\"predicate\": \"主演\", \"object_type\": \"人物\", \"subject_type\": \"影视作品\", \"object\": \"文章\", \"subject\": \"裸婚时代\"}, {\"predicate\": \"主演\", \"object_type\": \"人物\", \"subject_type\": \"影视作品\", \"object\": \"姚笛\", \"subject\": \"裸婚时代\"}, {\"predicate\": \"主演\", \"object_type\": \"人物\", \"subject_type\": \"影视作品\", \"object\": \"张凯丽\", \"subject\": \"裸婚时代\"}, {\"predicate\": \"主演\", \"object_type\": \"人物\", \"subject_type\": \"影视作品\", \"object\": \"丁嘉丽\", \"subject\": \"裸婚时代\"}, {\"predicate\": \"改编自\", \"object_type\": \"作品\", \"subject_type\": \"影视作品\", \"object\": \"裸婚——80后的新结婚时代\", \"subject\": \"裸婚时代\"}]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index:16866\n",
      "text:这本《康里巎巎杂诗(全彩色高清珍藏本)》(作者胡抗美)是其中一册\n",
      "\n",
      "NOREF:\n",
      "{   'object': '胡抗美',\n",
      "    'object_type': '人物',\n",
      "    'predicate': '作者',\n",
      "    'subject': '康里巎巎《杂诗》',\n",
      "    'subject_type': '图书作品'}\n",
      "index:32313\n",
      "text:吴磊《沙海》《斗破苍穹》吴磊的《沙海》，除了吴磊 ，还有秦昊 、杨蓉、张萌、张铭恩、朱杰 等人主演的现代探险题材电视剧，该剧改编自南派三叔同名小说，普通高中生黎簇被卷入一个以世界现状为目的庞大计划中，然后认识了这一切的幕后布局者、以旅行摄影作家关根身份登场的吴邪的故事\n",
      "\n",
      "NOREF:\n",
      "{   'object': '裸婚——80后的新结婚时代',\n",
      "    'object_type': '作品',\n",
      "    'predicate': '改编自',\n",
      "    'subject': '裸婚时代',\n",
      "    'subject_type': '影视作品'}\n",
      "index:82535\n",
      "text:2008年1月24日上午9点半，央视“百家讲坛”栏目主讲人王立群将携《王立群读史记之 项羽 》在北京长城饭店召开新书首发会，欢迎有兴趣的读者到场参加\n",
      "\n",
      "NOREF:\n",
      "{   'object': '王立群',\n",
      "    'object_type': '人物',\n",
      "    'predicate': '主持人',\n",
      "    'subject': '王立群读《史记》',\n",
      "    'subject_type': '电视综艺'}\n",
      "index:93654\n",
      "text:2010年9月，在《创:战纪》上映前，迪士尼曾聘请《无间行者》、《天国王朝》编剧威廉·莫纳汉为电影版改编剧本\n",
      "\n",
      "NOREF:\n",
      "{   'object': '白发魔女传',\n",
      "    'object_type': '作品',\n",
      "    'predicate': '改编自',\n",
      "    'subject': '白发魔女传',\n",
      "    'subject_type': '影视作品'}\n",
      "index:99891\n",
      "text:概述桂林国际会展中心位于素有百里画廊之称的漓江之滨，是桂林市标志性建筑，占地面积 15 万平方米，建筑总面积 5\n",
      "\n",
      "NOREF:\n",
      "{   'object': '15万平方米',\n",
      "    'object_type': 'Number',\n",
      "    'predicate': '占地面积',\n",
      "    'subject': '桂林国际会展中心',\n",
      "    'subject_type': '机构'}\n",
      "index:138087\n",
      "text:《千字文全解》是2009年人民文学出版社出版的图书，作者是李小龙\n",
      "\n",
      "NOREF:\n",
      "{   'object': '人民文学出版社',\n",
      "    'object_type': '出版社',\n",
      "    'predicate': '出版社',\n",
      "    'subject': '《千字文》全解',\n",
      "    'subject_type': '书籍'}\n",
      "index:152858\n",
      "text:周秦时代诗的传播史是社会科学文献出版社出版的，马银琴编写的图书\n",
      "\n",
      "NOREF:\n",
      "{   'object': '马银琴',\n",
      "    'object_type': '人物',\n",
      "    'predicate': '作者',\n",
      "    'subject': '周秦时代《诗》的传播史',\n",
      "    'subject_type': '图书作品'}\n",
      "\n",
      "NOREF:\n",
      "{   'object': '社会科学文献出版社',\n",
      "    'object_type': '出版社',\n",
      "    'predicate': '出版社',\n",
      "    'subject': '周秦时代《诗》的传播史',\n",
      "    'subject_type': '书籍'}\n",
      "index:167561\n",
      "text:《裸婚时代》改编自80后网络作家唐欣恬的小说《裸婚——80后的新结婚时代》，由北京光彩世纪文化艺术有限公司出品，导演滕华涛执导，文章首次担当编剧，文章、姚笛、张凯丽、丁嘉丽、韩童生、万茜等主演\n",
      "\n",
      "NOREF:\n",
      "{   'object': '山楂树之恋',\n",
      "    'object_type': '作品',\n",
      "    'predicate': '改编自',\n",
      "    'subject': '山楂树之恋',\n",
      "    'subject_type': '影视作品'}\n"
     ]
    }
   ],
   "source": [
    "for ind in indexs:\n",
    "    analy_spo(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train.csv', index=False, encoding='utf8')\n",
    "dev_df.to_csv('dev.csv', index=False, encoding='utf8')\n",
    "test_df.to_csv('test.csv', index=False, encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义三种spo实体关系：\n",
    "- 1. NORMAL：不包含下列情况\n",
    "- 2. OVERLAP：同一实体 有多种spo关系  \n",
    "- 3. MULTI-LABEL： 同一实体对 有多种关系\n",
    "- 4. NEST： 实体嵌套"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_normal(tri):\n",
    "    entities = set()\n",
    "    for t in tri:\n",
    "        entities.add(tuple(t[0]))\n",
    "        entities.add(tuple(t[1]))\n",
    "    return len(entities) == 2 * len(tri)\n",
    "\n",
    "def is_overlap(tri):\n",
    "    if is_normal(tri):\n",
    "        return False\n",
    "    entities_pair = set()\n",
    "    for t in tri:\n",
    "        entities_pair.add(tuple([t[0][0], t[0][1], t[1][0], t[1][1]]))\n",
    "    entities = set()\n",
    "    for ep in entities_pair:\n",
    "        entities.add(tuple([ep[0], ep[1]]))\n",
    "        entities.add(tuple([ep[2], ep[3]]))\n",
    "    return len(entities) != 2 * len(entities_pair)\n",
    "\n",
    "def is_multi(tri):\n",
    "    if is_normal(tri):\n",
    "        return False\n",
    "    entities_pair = list()\n",
    "    for t in tri:\n",
    "        entities_pair.append(tuple([t[0][0], t[0][1], t[1][0], t[1][1]]))\n",
    "    return len(set(entities_pair)) != len(entities_pair)\n",
    "\n",
    "def is_nest(tri):\n",
    "    entities = set()\n",
    "    for t in tri:\n",
    "        entities.add(tuple(t[0]))\n",
    "        entities.add(tuple(t[1]))\n",
    "#     entities_pos = list()\n",
    "#     for e in entities:\n",
    "#         entities_pos.append(e[0])\n",
    "#         entities_pos.append(e[1])\n",
    "#     return len(set(entities_pos)) != len(entities_pos)\n",
    "    entities = sorted(list(entities), key=lambda x: x[0])\n",
    "    end = -1\n",
    "    for e in entities:\n",
    "        if e[0] <= end:\n",
    "            return True\n",
    "        end = e[1]\n",
    "    return False\n",
    "\n",
    "def pro_entities_relation(df):\n",
    "    df['normal'] = df.apply(lambda x: is_normal(json.loads(x['spo'])), axis=1)\n",
    "    df['overlap'] = df.apply(lambda x: is_overlap(json.loads(x['spo'])), axis=1)\n",
    "    df['multi'] = df.apply(lambda x: is_multi(json.loads(x['spo'])), axis=1)\n",
    "    df['nest'] =df.apply(lambda x: is_nest(json.loads(x['spo'])), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pro_entities_relation(train_df)\n",
    "dev_df = pro_entities_relation(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'《如果我爱你》是由海润影视与明道工作室联合出品，徐辅军执导，明道、李沁、胡兵、白歆惠、狄杰等人气明星联袂主演的浪漫偶像剧'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['nest']==True].iloc[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在NEST状况中可以发现，一个实体可能会多次重复出现\n",
    "\n",
    "如:\n",
    ">'《如果我爱你》是由海润影视与明道工作室联合出品，徐辅军执导，明道、李沁、胡兵、白歆惠、狄杰等人气明星联袂主演的浪漫偶像剧'\n",
    "\n",
    "明道工作室和明道可能存在嵌套，而之后存在单独的实体明道。\n",
    "\n",
    "解决方案：\n",
    "1. cpoy -> generate\n",
    "2. 数据处理，若出现嵌套，向后继续搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_spo(df, index):\n",
    "    row = df.iloc[index]\n",
    "    text = row['text']\n",
    "    raw_spo = row['spo']\n",
    "    spo = json.loads(row['spo'])\n",
    "    entities = set()\n",
    "    for t in spo:\n",
    "        entities.add(tuple(t[0]))\n",
    "        entities.add(tuple(t[1]))\n",
    "    entities_list = sorted(list(entities), key=lambda x: (x[0], -(x[1]-x[0])))\n",
    "    end = -1\n",
    "    change_list = []\n",
    "    for e in entities_list:\n",
    "        if e[0] <= end:\n",
    "            new = text.find(text[e[0]:e[1]+1], e[0]+1)\n",
    "            if new != -1:\n",
    "                change_list.append((e, (new, new+e[1]-e[0])))\n",
    "        else:\n",
    "            end = e[1]\n",
    "    for change in change_list:\n",
    "        old = json.dumps(change[0])\n",
    "        new = json.dumps(change[1])\n",
    "        raw_spo = raw_spo.replace(old,new)\n",
    "        print(1)\n",
    "    df.loc[index, 'spo'] = raw_spo\n",
    "    \n",
    "def mod_df_spo(df):\n",
    "    dest_indexes = list(df[df['nest']==True].index)\n",
    "    print(f'BEFORE MOD DEST NUMBER:{len(dest_indexes)}')\n",
    "    for index in dest_indexes:\n",
    "        mod_spo(df, index)\n",
    "    df['nest'] =df.apply(lambda x: is_nest(json.loads(x['spo'])), axis=1)\n",
    "    print(\"AFTER MOD DEST NUMBER:{}\".format(len(list(df[df['nest']==True].index))))\n",
    "    print('REDUCE {}'.format(len(dest_indexes) - len(list(df[df['nest']==True].index))))\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE MOD DEST NUMBER:6096\n",
      "AFTER MOD DEST NUMBER:3676\n",
      "REDUCE 2420\n"
     ]
    }
   ],
   "source": [
    "train_df = mod_df_spo(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE MOD DEST NUMBER:719\n",
      "AFTER MOD DEST NUMBER:429\n",
      "REDUCE 290\n"
     ]
    }
   ],
   "source": [
    "dev_df = mod_df_spo(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE MOD DEST NUMBER:3676\n",
      "AFTER MOD DEST NUMBER:3661\n",
      "REDUCE 15\n"
     ]
    }
   ],
   "source": [
    "# LETS TRY AGAIM\n",
    "train_df = mod_df_spo(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE MOD DEST NUMBER:429\n",
      "AFTER MOD DEST NUMBER:423\n",
      "REDUCE 6\n"
     ]
    }
   ],
   "source": [
    "dev_df = mod_df_spo(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE MOD DEST NUMBER:3661\n",
      "AFTER MOD DEST NUMBER:3659\n",
      "REDUCE 2\n",
      "BEFORE MOD DEST NUMBER:423\n",
      "AFTER MOD DEST NUMBER:423\n",
      "REDUCE 0\n"
     ]
    }
   ],
   "source": [
    "# LETS TRY AGAIN AGAIN\n",
    "train_df = mod_df_spo(train_df)\n",
    "dev_df = mod_df_spo(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE MOD DEST NUMBER:3659\n",
      "AFTER MOD DEST NUMBER:3659\n",
      "REDUCE 0\n",
      "BEFORE MOD DEST NUMBER:423\n",
      "AFTER MOD DEST NUMBER:423\n",
      "REDUCE 0\n"
     ]
    }
   ],
   "source": [
    "# LETS TRY AGAIN AGAIN AGAIN\n",
    "train_df = mod_df_spo(train_df)\n",
    "dev_df = mod_df_spo(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[[1, 5], [9, 12], 17], [[1, 5], [33, 34], 0], [[1, 5], [24, 26], 4], [[1, 5], [39, 41], 0], [[1, 5], [14, 18], 17], [[1, 5], [43, 44], 0], [[1, 5], [36, 37], 0], [[1, 5], [30, 31], 0]]'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[42, 'spo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词错误率\n",
    "检测是否会由于分词造成实体不能正常识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_seg(words, text, spo):\n",
    "    \"\"\"\n",
    "    return:\n",
    "    -1 无官方分词结果\n",
    "    0  存在误分实体单词\n",
    "    1  不存在误分\n",
    "    \"\"\"\n",
    "    if len(words) == 0:\n",
    "        return -1\n",
    "    ws = words.split('\\n')\n",
    "    assert len(ws) + len(text) - 1 == len(words), f'{len(text)}\\n{len(words)}\\n{len(ws)}'\n",
    "    entities = set()\n",
    "    for t in spo:\n",
    "        entities.add(tuple(t[0]))\n",
    "        entities.add(tuple(t[1]))\n",
    "    points = set()\n",
    "    begin = 0\n",
    "    for w in ws:\n",
    "        points.add(begin)\n",
    "        points.add(begin + len(w) - 1)\n",
    "        begin += len(w)\n",
    "    for e in entities:\n",
    "        if e[0] not in points or e[1] not in points:\n",
    "            return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_df['check_seg'] = dev_df.apply(lambda x: check_seg(x['words'], x['text'], json.loads(x['spo'])), axis=1)\n",
    "train_df['check_seg'] = train_df.apply(lambda x: check_seg(x['words'], x['text'], json.loads(x['spo'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28530"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['check_seg']==0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[99891, 'postag']='[{\"word\": \"概述\", \"pos\": \"v\"}, {\"word\": \"桂林国际会展中心\", \"pos\": \"ns\"}, {\"word\": \"位于\", \"pos\": \"v\"}, {\"word\": \"素有\", \"pos\": \"v\"}, {\"word\": \"百里\", \"pos\": \"m\"}, {\"word\": \"画廊\", \"pos\": \"n\"}, {\"word\": \"之\", \"pos\": \"u\"}, {\"word\": \"称\", \"pos\": \"n\"}, {\"word\": \"的\", \"pos\": \"u\"}, {\"word\": \"漓江\", \"pos\": \"ns\"}, {\"word\": \"之\", \"pos\": \"u\"}, {\"word\": \"滨\", \"pos\": \"n\"}, {\"word\": \"，\", \"pos\": \"w\"}, {\"word\": \"是\", \"pos\": \"v\"}, {\"word\": \"桂林市\", \"pos\": \"ns\"}, {\"word\": \"标志性\", \"pos\": \"n\"}, {\"word\": \"建筑\", \"pos\": \"n\"}, {\"word\": \"，\", \"pos\": \"w\"}, {\"word\": \"占地面积\", \"pos\": \"n\"}, {\"word\": \"15万平方米\", \"pos\": \"m\"}, {\"word\": \"，\", \"pos\": \"w\"}, {\"word\": \"建筑\", \"pos\": \"n\"}, {\"word\": \"总\", \"pos\": \"a\"}, {\"word\": \"面积\", \"pos\": \"n\"},{\"word\": \"5\", \"pos\": \"m\"}]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[99891 , 'words'] = '概述\\n桂林国际会展中心\\n位于\\n素有\\n百里\\n画廊\\n之\\n称\\n的\\n漓江\\n之\\n滨\\n，\\n是\\n桂林市\\n标志性\\n建筑\\n，\\n占地面积\\n15万平方米\\n，\\n建筑\\n总\\n面积\\n5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'马志舟，1907年出生，陕西三原人，汉族，中国共产党，任红四团第一连连长，1933年逝世'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df[dev_df['check_seg']==0].loc[8, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'马志舟\\n，\\n1907年\\n出生\\n，\\n陕西\\n三原\\n人\\n，\\n汉族\\n，\\n中国共产党\\n，\\n任\\n红四团第一连\\n连长\\n，\\n1933年\\n逝世'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df[dev_df['check_seg']==0].loc[8, 'words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"predicate\": \"国籍\", \"object_type\": \"国家\", \"subject_type\": \"人物\", \"object\": \"中国\", \"subject\": \"马志舟\"}, {\"predicate\": \"出生日期\", \"object_type\": \"Date\", \"subject_type\": \"人物\", \"object\": \"1907年\", \"subject\": \"马志舟\"}, {\"predicate\": \"民族\", \"object_type\": \"Text\", \"subject_type\": \"人物\", \"object\": \"汉族\", \"subject\": \"马志舟\"}, {\"predicate\": \"出生地\", \"object_type\": \"地点\", \"subject_type\": \"人物\", \"object\": \"陕西三原\", \"subject\": \"马志舟\"}]'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df[dev_df['check_seg']==0].loc[8, 'spo_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = '查尔斯·阿兰基斯（Charles Aránguiz），1989年4月17日出生于智利圣地亚哥，智利职业足球运动员，司职中场，效力于德国足球甲级联赛勒沃库森足球俱乐部'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = '查尔斯\\n·\\n阿兰基斯\\n（\\nCharles Aránguiz\\n）\\n，\\n1989年4月17日\\n出生\\n于\\n智利圣地亚哥\\n，\\n智利\\n职业\\n足球\\n运动员\\n，\\n司职\\n中场\\n，\\n效力\\n于\\n德国\\n足球\\n甲级\\n联赛\\n勒沃库森足球俱乐部'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s2.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 0\n",
    "for w in s2.split('\\n'):\n",
    "    l += len(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'查尔斯\\n·\\n阿兰基斯\\n（\\nCharles Aránguiz\\n）\\n，\\n1989年4月17日\\n出生\\n于\\n智利圣地亚哥\\n，\\n智利\\n职业\\n足球\\n运动员\\n，\\n司职\\n中场\\n，\\n效力\\n于\\n德国\\n足球\\n甲级\\n联赛\\n勒沃库森足球俱乐部查尔斯\\n·\\n阿兰基斯\\n（\\nCharles Aránguiz\\n）\\n，\\n1989年4月17日\\n出生\\n于\\n智利圣地亚哥\\n，\\n智利\\n职业\\n足球\\n运动员\\n，\\n司职\\n中场\\n，\\n效力\\n于\\n德国\\n足球\\n甲级\\n联赛\\n勒沃库森足球俱乐部'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
